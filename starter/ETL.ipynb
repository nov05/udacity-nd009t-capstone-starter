{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* notebook created by nov05 on 2025-01-12  \n",
    "* Registry of Open Data on AWS: [**Amazon Bin Image Dataset**](https://registry.opendata.aws/amazon-bin-imagery/)      \n",
    "  https://us-east-1.console.aws.amazon.com/s3/buckets/aft-vbi-pds  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## windows cmd to launch notepad to edit aws config and credential files\n",
    "# !notepad C:\\Users\\guido\\.aws\\config\n",
    "!notepad C:\\Users\\guido\\.aws\\credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘‰ **Download metadata from S3** \n",
    "\n",
    "Download a portion of the metadata from the public S3 bucket containing the **Amazon Bin Image Dataset** to your local system.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example code to download a file from s3 bucket\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "# Create an S3 client with unsigned requests (public access)\n",
    "s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "s3_client.download_file(\n",
    "    Bucket='aft-vbi-pds',\n",
    "    Key='bin-images/100313.jpg',\n",
    "    Filename='../data/bin-images/100313.jpg'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "def download_and_arrange_data(\n",
    "        prefix='bin-images', \n",
    "        file_extension='.jpg',\n",
    "        download_dir='../data/train',\n",
    "        partition=True):\n",
    "    \n",
    "    s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "    ## There are 140536 image file names in the list. \n",
    "    with open('file_list.json', 'r') as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    for k, v in d.items():  ## There are 5 items (for 5 classes) in the JSON file.\n",
    "        print(f\"Downloading images/metadata of images with {k} object...\")\n",
    "        if partition:\n",
    "            download_dir = os.path.join(download_dir, k)\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "        for file_path in tqdm(v):\n",
    "            file_name = os.path.basename(file_path).split('.')[0] + file_extension\n",
    "            s3_client.download_file(\n",
    "                'aft-vbi-pds', \n",
    "                prefix+'/'+file_name,  ## e.g. metadata/100313.json\n",
    "                download_dir+'/'+file_name)\n",
    "            \n",
    "## download metadata, 17.9 MB, 56m 57.4s\n",
    "download_and_arrange_data(\n",
    "    prefix='metadata', \n",
    "    file_extension='.json',\n",
    "    download_dir='../data/metadata',\n",
    "    partition=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Downloading images/metadata of images with 1 object...\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1228/1228 [06:36<00:00,  3.09it/s]\n",
    "Downloading images/metadata of images with 2 object...\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2299/2299 [12:38<00:00,  3.03it/s]\n",
    "Downloading images/metadata of images with 3 object...\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2666/2666 [14:35<00:00,  3.04it/s]\n",
    "Downloading images/metadata of images with 4 object...\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2373/2373 [12:54<00:00,  3.06it/s]\n",
    "Downloading images/metadata of images with 5 object...\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [10:11<00:00,  3.07it/s]  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total metadata file number: 10441\n"
     ]
    }
   ],
   "source": [
    "print(\"total metadata file number:\", 1228 + 2299 + 2666 + 2373 + 1875)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘‰ **Upload metadata to S3**\n",
    "\n",
    "Upload this portion of the metadata to my own S3 bucket for further experimental analysis using AWS Glue, Athena, and other services.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example code: upload a file to s3. mind the profile that is used.\n",
    "import boto3\n",
    "session = boto3.Session(profile_name='admin')  ## use the profile name in the credentials file\n",
    "s3_client = session.client('s3')\n",
    "bucket = 'dataset-aft-vbi-pds'\n",
    "key = 'metadata/100313.json'\n",
    "filename = '../data/metadata/100313.json'\n",
    "s3_client.upload_file(\n",
    "    Filename=filename,\n",
    "    Key=key,\n",
    "    Bucket=bucket\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/metadata []\n",
      "../data/metadata\\00004.json\n",
      "metadata\\00004.json\n"
     ]
    }
   ],
   "source": [
    "## example code of directory traversal\n",
    "import os\n",
    "local_folder = '../data/metadata'\n",
    "for root, dir, files in os.walk(local_folder):\n",
    "    print(root, dir)\n",
    "    for i,file in enumerate(files):\n",
    "        local_file = os.path.join(root, file)\n",
    "        print(local_file)\n",
    "        relative_path = os.path.relpath(local_file, '../data/')\n",
    "        print(relative_path)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10445/10445 [53:23<00:00,  3.26it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "def upload_folder_to_s3(local_folder, bucket_name, s3_folder=''):\n",
    "    session = boto3.Session(profile_name='admin')  ## use the profile name in the credentials file\n",
    "    s3_client = session.client('s3')\n",
    "    for root, _, files in os.walk(local_folder):\n",
    "        for file in tqdm(files):\n",
    "            local_file = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_file, local_folder)  # Get relative file path\n",
    "            s3_file = os.path.join(s3_folder, relative_path).replace(\"\\\\\", \"/\")  # Handle folder structure in S3\n",
    "            try:\n",
    "                s3_client.upload_file(local_file, bucket_name, s3_file)\n",
    "            except NoCredentialsError:\n",
    "                print(\"AWS credentials not available.\")\n",
    "bucket = 'dataset-aft-vbi-pds'\n",
    "local_folder = '../data/metadata/'  # Local folder path\n",
    "s3_folder = 'metadata/'  # The folder in S3 to upload to (optional)\n",
    "upload_folder_to_s3(local_folder, bucket, s3_folder)\n",
    "## 53m 23.5s for uploading 10445 json files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awsmle_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
