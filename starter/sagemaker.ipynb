{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notebook created by nov05 on 2024-12-29\n",
    "* It was run locally with conda env `sagemaker_py310`.  \n",
    "\n",
    "---   \n",
    "* View the S3 bucket in your account   \n",
    "    https://s3.console.aws.amazon.com/s3/buckets/aft-vbi-pds\n",
    "* [Docs > Models and pre-trained weights > ResNet > resnet34](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet34.html)  \n",
    "* GitHub gist [code snippets](https://gist.github.com/nov05/95cb7edcbe2e8bb68c9d29bdc00b9ca8)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\github\\\\udacity-nd009t-capstone-starter\\\\starter'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## windows cmd to launch notepad to edit aws credential file\n",
    "# !notepad C:\\Users\\guido\\.aws\\config\n",
    "!notepad C:\\Users\\guido\\.aws\\credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/31/25 16:07:16] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\botocore\\credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\botocore\\credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/31/25 16:07:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=447278;file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\botocore\\credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=900509;file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\botocore\\credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/31/25 16:07:19] </span><span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">WARNING </span> Couldn't call <span style=\"color: #008700; text-decoration-color: #008700\">'get_role'</span> to get Role ARN from role name voclabs to get <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\session.py#5971\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5971</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Role path.                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/31/25 16:07:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;175;0mWARNING \u001b[0m Couldn't call \u001b[38;2;0;135;0m'get_role'\u001b[0m to get Role ARN from role name voclabs to get \u001b]8;id=280679;file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=330575;file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\session.py#5971\u001b\\\u001b[2m5971\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         Role path.                                                             \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current AWS Account ID: 570668189909\n",
      "AWS Region: us-east-1\n",
      "Default Bucket: sagemaker-us-east-1-570668189909\n",
      "Role voclabs ARN: arn:aws:iam::570668189909:role/voclabs\n",
      "SageMaker Role ARN: arn:aws:iam::570668189909:role/service-role/AmazonSageMaker-ExecutionRole-20250126T194519\n"
     ]
    }
   ],
   "source": [
    "## reset the session after updating credentials\n",
    "import boto3 # type: ignore\n",
    "boto3.DEFAULT_SESSION = None\n",
    "import sagemaker # type: ignore\n",
    "from sagemaker import get_execution_role # type: ignore\n",
    "\n",
    "# Extract and print the account ID\n",
    "sts_client = boto3.client('sts')\n",
    "response = sts_client.get_caller_identity() \n",
    "account_id = response['Account']\n",
    "\n",
    "role_arn = get_execution_role()  ## get role ARN\n",
    "if 'AmazonSageMaker-ExecutionRole' not in role_arn:\n",
    "    ## Go to \"IAM - Roles\", search for \"SageMaker\", find the execution role.\n",
    "    voclabs_role_arn = role_arn\n",
    "    sagemaker_role_arn = \"arn:aws:iam::570668189909:role/service-role/AmazonSageMaker-ExecutionRole-20250126T194519\"\n",
    "session = sagemaker.Session()  ## \"default\"\n",
    "region = session.boto_region_name\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "print(f\"Current AWS Account ID: {account_id}\")\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "print(\"Default Bucket: {}\".format(bucket))\n",
    "print(f\"Role voclabs ARN: {voclabs_role_arn}\") ## If local, Role ARN: arn:aws:iam::807711953667:role/voclabs\n",
    "print(\"SageMaker Role ARN: {}\".format(sagemaker_role_arn)) \n",
    "\n",
    "## generate secrets.env. remember to add it to .gitignore  \n",
    "import wandb\n",
    "wandb.sagemaker_auth(path=\"../secrets\") \n",
    "\n",
    "## get my own AWS account number\n",
    "with open('../secrets/aws_account_number', 'r') as file:\n",
    "    for line in file:\n",
    "        aws_account_number = line.strip()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëâ **Data Preparation**\n",
    "**TODO:** Run the cell below to download the data.\n",
    "\n",
    "The cell below creates a folder called `data`, downloads training data and arranges it in subfolders. Each of these subfolders contain images where the number of objects is equal to the name of the folder. For instance, all images in folder `1` has images with 1 object in them. Images are not divided into training, testing or validation sets. If you feel like the number of samples are not enough, you can always download more data (instructions for that can be found [here](https://registry.opendata.aws/amazon-bin-imagery/)). However, we are not assessing you on the accuracy of your final trained model, but how you create your machine learning engineering pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* View the S3 bucket in your account   \n",
    "    https://s3.console.aws.amazon.com/s3/buckets/aft-vbi-pds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example code to download a file from s3 bucket\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "# Create an S3 client with unsigned requests (public access)\n",
    "s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "s3_client.download_file(\n",
    "    Bucket='aft-vbi-pds',\n",
    "    Key='bin-images/100313.jpg',\n",
    "    Filename='../data/bin-images/100313.jpg'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images/metadata of images with 1 object...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1228/1228 [06:36<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images/metadata of images with 2 object...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2299/2299 [12:38<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images/metadata of images with 3 object...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2666/2666 [14:35<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images/metadata of images with 4 object...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2373/2373 [12:54<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images/metadata of images with 5 object...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1875/1875 [10:11<00:00,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "def download_and_arrange_data(\n",
    "        prefix='bin-images', \n",
    "        file_extension='.jpg',\n",
    "        download_dir='../data/bin-images',\n",
    "        partition=True):\n",
    "    \n",
    "    s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))  ## public access\n",
    "\n",
    "    ## There are 140536 image file names in the list. \n",
    "    with open('file_list.json', 'r') as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    for k, v in d.items():  ## There are 5 items (for 5 classes) in the JSON file.\n",
    "        print(f\"Downloading images/metadata of images with {k} object...\")\n",
    "        if partition:\n",
    "            download_dir = os.path.join(download_dir, k)\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "        for file_path in tqdm(v):\n",
    "            file_name = os.path.basename(file_path).split('.')[0] + file_extension\n",
    "            s3_client.download_file(\n",
    "                'aft-vbi-pds', \n",
    "                prefix+'/'+file_name,  ## e.g. metadata/100313.json\n",
    "                download_dir+'/'+file_name)\n",
    "            \n",
    "## download metadata, 17.9 MB, 56m 57.4s\n",
    "download_and_arrange_data(\n",
    "    prefix='metadata', \n",
    "    file_extension='.json',\n",
    "    download_dir='../data/metadata',\n",
    "    partition=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total metadata file number: 10441\n"
     ]
    }
   ],
   "source": [
    "print(\"total metadata file number:\", 1228 + 2299 + 2666 + 2373 + 1875)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/31/25 05:43:31] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Skipping checksum validation. Response did not contain one of the  <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\botocore\\httpchecksum.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">httpchecksum.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\botocore\\httpchecksum.py#481\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">481</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         following algorithms: <span style=\"font-weight: bold\">[</span><span style=\"color: #008700; text-decoration-color: #008700\">'crc32'</span>, <span style=\"color: #008700; text-decoration-color: #008700\">'sha1'</span>, <span style=\"color: #008700; text-decoration-color: #008700\">'sha256'</span><span style=\"font-weight: bold\">]</span>.                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/31/25 05:43:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Skipping checksum validation. Response did not contain one of the  \u001b]8;id=399315;file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\botocore\\httpchecksum.py\u001b\\\u001b[2mhttpchecksum.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=725570;file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\botocore\\httpchecksum.py#481\u001b\\\u001b[2m481\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         following algorithms: \u001b[1m[\u001b[0m\u001b[38;2;0;135;0m'crc32'\u001b[0m, \u001b[38;2;0;135;0m'sha1'\u001b[0m, \u001b[38;2;0;135;0m'sha256'\u001b[0m\u001b[1m]\u001b[0m.                 \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total image file number: 10441\n",
      "Example file list: [('03146', '4'), ('102813', '4'), ('100517', '5'), ('08340', '4'), ('06487', '2')]\n"
     ]
    }
   ],
   "source": [
    "## example code to get jpg-json file name pairs \n",
    "import json\n",
    "import random\n",
    "def get_file_list(s3_uri):\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket, key = s3_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    json_content = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "    file_list = []\n",
    "    for label, file_name_list in json_content.items():\n",
    "        for file_name in file_name_list:\n",
    "            file_list.append((file_name.split(\"/\")[-1].split(\".\")[0], label))\n",
    "    random.shuffle(file_list)\n",
    "    return file_list  \n",
    "FILE_LIST_KEY = \"s3://p5-amazon-bin-images/file_list.json\"\n",
    "file_list = get_file_list(FILE_LIST_KEY)\n",
    "print(\"total image file number:\", len(file_list))\n",
    "print(\"Example file list:\", file_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëâ WebDataset version: 0.2.100\n",
      "100313.image\n",
      "100313.metadata\n"
     ]
    }
   ],
   "source": [
    "## example code for webdataset\n",
    "import io\n",
    "import webdataset as wds\n",
    "print(\"üëâ WebDataset version:\", wds.__version__)\n",
    "tar_stream = io.BytesIO()\n",
    "base_name = \"100313\"\n",
    "with wds.TarWriter(tar_stream) as sink:\n",
    "    with open(\"../data/bin-images/100313.jpg\", \"rb\") as f:\n",
    "        image_data = f.read()\n",
    "    with open(\"../data/metadata/100313.json\", \"rb\") as f:\n",
    "        metadata_data = f.read()\n",
    "    # Save as WebDataset sample\n",
    "    sink.write({\n",
    "        \"__key__\": f\"{base_name}\",\n",
    "        \"image\": image_data,\n",
    "        \"metadata\": metadata_data\n",
    "    })\n",
    "# Once the tar file is in memory, save to local file\n",
    "tar_stream.seek(0)\n",
    "with open(\"../data/test/test.tar\", \"wb\") as f:\n",
    "    f.write(tar_stream.getvalue())\n",
    "!tar -tf ../data/test/test.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [ScriptProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ScriptProcessor) official documentation    \n",
    "* [My tutorial](https://docs.google.com/document/d/17KzWVf84xQJVNH1jd6yh_FLgr781QcdKng1JIF6P5X4): Create custom docker image for SageMaker data processing jobs, create AWS ECR private repo, and upload the image to the repo   \n",
    "* [AWS re:Post](https://repost.aws/en/knowledge-center/secondary-account-access-ecr), pull ECR image from the repo of another account  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='docker login --username AWS --password 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 https://570668189909.dkr.ecr.us-east-1.amazonaws.com', returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "There is no need to run this cell. Just update permissions of the ECR repo \n",
    "to allow pulling from another AWS account, and add the \"AmazonEC2ContainerRegistryPowerUser\"\n",
    "policy permissions to the SageMaker role of this account.\n",
    "'''\n",
    "# ## To pull ECR image from another AWS account \n",
    "# import boto3\n",
    "# import subprocess\n",
    "# import base64\n",
    "# ecr_client = boto3.client('ecr', region_name='us-east-1')\n",
    "# # Retrieve the authentication token from ECR\n",
    "# response = ecr_client.get_authorization_token()\n",
    "# authorization_data = response['authorizationData'][0]\n",
    "# token = authorization_data['authorizationToken']\n",
    "# registry_uri = authorization_data['proxyEndpoint']\n",
    "# decoded_token = base64.b64decode(token).decode('utf-8')\n",
    "# username, password = decoded_token.split(':')\n",
    "# # Docker login command\n",
    "# login_command = f\"docker login --username {username} --password {password} {registry_uri}\"\n",
    "# subprocess.run(login_command, shell=True, check=True)\n",
    "# # Now you can use this image in your SageMaker processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/30/25 20:17:20] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating processing-job with name                                      <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\session.py#1575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1575</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         p5-amazon-bin-images-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-01-31-02-17-16-724                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/30/25 20:17:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating processing-job with name                                      \u001b]8;id=766417;file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=536363;file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\session.py#1575\u001b\\\u001b[2m1575\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         p5-amazon-bin-images-\u001b[1;36m2025\u001b[0m-01-31-02-17-16-724                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......üëâ image_keys: ['bin-images/', 'bin-images/00001.jpg', 'bin-images/00002.jpg', 'bin-images/00003.jpg', 'bin-images/00004.jpg', 'bin-images/00005.jpg', 'bin-images/00006.jpg', 'bin-images/00007.jpg', 'bin-images/00008.jpg', 'bin-images/00009.jpg']\n",
      "‚ö†Ô∏è Skipping non-image file: bin-images/\n",
      "üü¢ Successfully uploaded tar file to s3://p5-amazon-bin-images/webdataset/data_0.tar\n",
      "üëâ image_keys: ['bin-images/00010.jpg', 'bin-images/00011.jpg', 'bin-images/00012.jpg', 'bin-images/00013.jpg', 'bin-images/00014.jpg', 'bin-images/00015.jpg', 'bin-images/00016.jpg', 'bin-images/00017.jpg', 'bin-images/00018.jpg', 'bin-images/00019.jpg']\n",
      "üü¢ Successfully uploaded tar file to s3://p5-amazon-bin-images/webdataset/data_1.tar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## example code for webdataset.TarWrite() conversion\n",
    "## this cell reads the first 20 jpg-json pairs and write them to 2 tar files\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    ## You can use a custom image or use the default SageMaker image\n",
    "    ## You can pull from AWS ECR or DockerHub\n",
    "    image_uri=f'{aws_account_number}.dkr.ecr.us-east-1.amazonaws.com/udacity/p5-amazon-bin-images:latest', \n",
    "    role=sagemaker_role_arn,  # Execution role\n",
    "    instance_count=1,\n",
    "    instance_type='ml.t3.large',  # Use the appropriate instance type\n",
    "    volume_size_in_gb=10,  # Minimal disk space since we're streaming\n",
    "    base_job_name='p5-amazon-bin-images' \n",
    ")\n",
    "processor.run(\n",
    "    ## ‚ö†Ô∏è I made a terrible mistake here by naming the script as \"webdataset.py\" \n",
    "    ## which is the same as the package name. You know what happened next.\n",
    "    code='../scripts_process/test_convert_to_webdataset.py',  # Your script to process data\n",
    "    arguments=[\n",
    "        '--SM_INPUT_BUCKET', 'aft-vbi-pds',\n",
    "        '--SM_INPUT_PREFIX_IMAGES', 'bin-images/',\n",
    "        '--SM_INPUT_PREFIX_METADATA', 'metadata/',\n",
    "        '--SM_OUTPUT_BUCKET', 'p5-amazon-bin-images',\n",
    "        '--SM_OUTPUT_PREFIX', 'webdataset/',\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job status: Completed\n"
     ]
    }
   ],
   "source": [
    "job_name = processor.latest_job.job_name\n",
    "processing_job_desc = processor.sagemaker_session.describe_processing_job(job_name)\n",
    "job_status = processing_job_desc['ProcessingJobStatus']\n",
    "print(f\"Processing job status: {job_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/31/25 17:55:50] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating processing-job with name                                      <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\session.py#1575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1575</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         p5-amazon-bin-images-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-01-31-23-55-46-542                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/31/25 17:55:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating processing-job with name                                      \u001b]8;id=28680;file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=800486;file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\session.py#1575\u001b\\\u001b[2m1575\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         p5-amazon-bin-images-\u001b[1;36m2025\u001b[0m-01-31-23-55-46-542                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................Starting data processing...\n",
      "üü¢ File list successfully loaded from s3://p5-amazon-bin-images/file_list.json\n",
      "    Total number of image files: 10441\n",
      "# writing train-shard-000000.tar 0 0.0 GB 0\n",
      "# writing train-shard-000001.tar 1000 0.1 GB 1000\n",
      "# writing train-shard-000002.tar 1000 0.1 GB 2000\n",
      "# writing train-shard-000003.tar 1000 0.1 GB 3000\n",
      "# writing train-shard-000004.tar 1000 0.1 GB 4000\n",
      "# writing train-shard-000005.tar 1000 0.1 GB 5000\n",
      "# writing train-shard-000006.tar 1000 0.1 GB 6000\n",
      "# writing train-shard-000007.tar 1000 0.1 GB 7000\n",
      "üü¢ Successfully uploaded shard files to s3://p5-amazon-bin-images/webdataset/train/:\n",
      "    ['train-shard-000000.tar', 'train-shard-000001.tar', 'train-shard-000002.tar', 'train-shard-000003.tar', 'train-shard-000004.tar', 'train-shard-000005.tar', 'train-shard-000006.tar', 'train-shard-000007.tar']\n",
      "# writing val-shard-000000.tar 0 0.0 GB 0\n",
      "# writing val-shard-000001.tar 1000 0.1 GB 1000\n",
      "üü¢ Successfully uploaded shard files to s3://p5-amazon-bin-images/webdataset/val/:\n",
      "    ['val-shard-000000.tar', 'val-shard-000001.tar']\n",
      "# writing test-shard-000000.tar 0 0.0 GB 0\n",
      "# writing test-shard-000001.tar 1000 0.1 GB 1000\n",
      "üü¢ Successfully uploaded shard files to s3://p5-amazon-bin-images/webdataset/test/:\n",
      "    ['test-shard-000000.tar', 'test-shard-000001.tar']\n",
      "\n",
      "CPU times: total: 27.6 s\n",
      "Wall time: 15min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIt took about 13 minutes to process 10.4K files (1.2 GB). If we keep 1K files per shard, \\nprocessing 500K files could take around 11 hours. I‚Äôll probably increase it to 10K \\nfiles per shard, which would make each tar file around 1 GB and speed up the process.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## TODO: Perform any data cleaning or data preprocessing\n",
    "## This cell shuffle then split the 10K dataset to train, val, and test.  \n",
    "## And convert the datasets to WebDataset tar files for SageMaker FastFile input mode.\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    ## You can use a custom image or use the default SageMaker image\n",
    "    ## You can pull from AWS ECR or DockerHub\n",
    "    image_uri=f'{aws_account_number}.dkr.ecr.us-east-1.amazonaws.com/udacity/p5-amazon-bin-images:latest', \n",
    "    role=sagemaker_role_arn,  # Execution role\n",
    "    instance_count=1,\n",
    "    instance_type='ml.t3.large',  # Use the appropriate instance type\n",
    "    volume_size_in_gb=10,  # Minimal disk space since we're streaming\n",
    "    base_job_name='p5-amazon-bin-images' \n",
    ")\n",
    "processor.run(\n",
    "    code='../scripts_process/convert_to_webdataset_10k.py',  # process the 10K files in the list\n",
    "    arguments=[\n",
    "        '--SM_INPUT_BUCKET', 'aft-vbi-pds',\n",
    "        '--SM_INPUT_PREFIX_IMAGES', 'bin-images/',\n",
    "        '--SM_INPUT_PREFIX_METADATA', 'metadata/',\n",
    "        '--SM_OUTPUT_BUCKET', 'p5-amazon-bin-images',\n",
    "        '--SM_OUTPUT_PREFIX', 'webdataset/',\n",
    "    ]\n",
    ")\n",
    "## It took about 13 minutes to process 10.4K files (1.2 GB). If we keep 1K files per shard, \n",
    "## processing 500K files could take around 11 hours. I‚Äôll probably increase it to 10K \n",
    "## files per shard, which would make each tar file around 1 GB and speed up the process.\n",
    "## CPU times: total: 21.9 s\n",
    "## Wall time: 12min 58s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëâ **Dataset**  \n",
    "\n",
    "**TODO:** Explain what dataset you are using for this project. Give a small overview of the classes, class distributions etc that can help anyone not familiar with the dataset get a better understanding of it. You can find more information about the data [here](https://registry.opendata.aws/amazon-bin-imagery/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "data_base_path = \"s3://p5-amazon-bin-images/webdataset/\"\n",
    "train_data = TrainingInput(data_base_path+\"train/train-shard-{{000000..000007}}.tar\", content_type=\"image/jpeg\")\n",
    "val_data = TrainingInput(data_base_path+\"valid/valid-shard-{{000000..000001}}.tar\", content_type=\"image/jpeg\")\n",
    "test_data = TrainingInput(data_base_path+\"test/test-shard-{{000000..000001}}.tar\", content_type=\"image/jpeg\")\n",
    "## ‚ö†Ô∏è don't use prefix in output_path, cause source folder will be created \n",
    "## at bucket level, while other folders, e.g. debug-output, at prefix levle.\n",
    "output_path = \"s3://p5-amazon-bin-images/\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëâ **Model Training (Distributed)**  \n",
    "\n",
    "**TODO:** This is the part where you can train a model. The type or architecture of the model you use is not important.   \n",
    "**Note:** You will need to use the `train.py` script to train your model.\n",
    "\n",
    "* Official document: [SageMaker distributed data parallel (SDP) with PyTorch](https://sagemaker-examples.readthedocs.io/en/latest/training/distributed_training/index.html#pytorch-distributed)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/31/25 19:35:53] </span><span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">WARNING </span> Framework profiling will be deprecated from tensorflow <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.12</span> and     <a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\deprecations.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">deprecations.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\deprecations.py#34\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         pytorch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span> in sagemaker&gt;=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         See: <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/v2.html</span> for         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         details.                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/31/25 19:35:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;175;0mWARNING \u001b[0m Framework profiling will be deprecated from tensorflow \u001b[1;36m2.12\u001b[0m and     \u001b]8;id=539974;file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\deprecations.py\u001b\\\u001b[2mdeprecations.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=770420;file://d:\\Users\\guido\\miniconda3\\envs\\sagemaker_py310\\lib\\site-packages\\sagemaker\\deprecations.py#34\u001b\\\u001b[2m34\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         pytorch \u001b[1;36m2.0\u001b[0m in sagemaker>=\u001b[1;36m2\u001b[0m.                                        \u001b[2m                  \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         See: \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/v2.html\u001b[0m for         \u001b[2m                  \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         details.                                                            \u001b[2m                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO: Declare your model training hyperparameter.\n",
    "#NOTE: You do not need to do hyperparameter tuning. You can use fixed hyperparameter values\n",
    "from sagemaker.debugger import (\n",
    "    Rule,\n",
    "    ## debugger\n",
    "    DebuggerHookConfig,\n",
    "    rule_configs,\n",
    "    ## profiler \n",
    "    ProfilerRule,\n",
    "    ProfilerConfig,\n",
    "    FrameworkProfile\n",
    ")\n",
    "hyperparameters = {\n",
    "    'epochs': 20,   \n",
    "    'batch-size': 32,   \n",
    "    'opt-learning-rate': 8e-5,  \n",
    "    'opt-weight-decay': 1e-5,  \n",
    "    'lr-sched-step-size': 5,  \n",
    "    'lr-sched-gamma': 0.5,\n",
    "    'early-stopping-patience': 5,\n",
    "    'model-type': 'resnet50', \n",
    "    'wandb': True,  \n",
    "    'debug': True, \n",
    "}\n",
    "rules = [\n",
    "    Rule.sagemaker(rule_configs.poor_weight_initialization()),\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "    ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "]\n",
    "hook_config = DebuggerHookConfig(\n",
    "    hook_parameters={\n",
    "        \"train.save_interval\": \"100\", \n",
    "        \"eval.save_interval\": \"10\"\n",
    "    }\n",
    ")\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500, \n",
    "    framework_profile_params=FrameworkProfile(num_steps=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check [SageMaker AI Pricing](https://aws.amazon.com/sagemaker-ai/pricing/) > On-Demand Pricing > Training  \n",
    "    | Instance Type      | vCPU | Memory  | Price per Hour |\n",
    "    |--------------------|------|---------|----------------|\n",
    "    | ml.g4dn.xlarge      | 4    | 16 GiB  | $0.736         |\n",
    "    |ml.p3.2xlarge\t| 8\t| 61 GiB\t| $3.825 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create your training estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point='train.py',  # Your training script that defines the ResNet50 model and training loop\n",
    "    source_dir='scripts_train',  # Directory where your script and dependencies are stored\n",
    "    role=sagemaker_role_arn,\n",
    "    framework_version='1.13.1',  # Use the PyTorch version you need\n",
    "    py_version='py39',\n",
    "    instance_count=2,  \n",
    "    # instance_type='ml.p3.2xlarge',  ## 16GB, Use GPU instances for deep learning\n",
    "    instance_type='ml.g4dn.xlarge',  ## 16GB\n",
    "    output_path=output_path,  ## if not specify, output to the sagemaker default bucket\n",
    "    hyperparameters=hyperparameters,\n",
    "    # use_spot_instances=True,\n",
    "    ## Debugger and profiler parameters\n",
    "    # rules=rules,\n",
    "    # debugger_hook_config=hook_config,    \n",
    "    # profiler_config=profiler_config,\n",
    "    ## Training using SMDataParallel Distributed Training Framework\n",
    "    distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO: Fit your estimator\n",
    "from datetime import datetime\n",
    "estimator.fit(\n",
    "    wait=True,  \n",
    "    job_name=f\"p5-amazon-bin-job-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",  \n",
    "    inputs={\n",
    "        \"train\": train_data,  \n",
    "        \"validation\": val_data, \n",
    "        \"test\": test_data,\n",
    "    },  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standout Suggestions\n",
    "You do not need to perform the tasks below to finish your project. However, you can attempt these tasks to turn your project into a more advanced portfolio piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "**TODO:** Here you can perform hyperparameter tuning to increase the performance of your model. You are encouraged to \n",
    "- tune as many hyperparameters as you can to get the best performance from your model\n",
    "- explain why you chose to tune those particular hyperparameters and the ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create your hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create your training estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit your estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Profiling and Debugging\n",
    "**TODO:** Use model debugging and profiling to better monitor and debug your model training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up debugging and profiling rules and hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and fit an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a debugging output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
    "**TODO**: If not, suppose there was an error. What would that error look like and how would you have fixed it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the profiler output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deploying and Querying\n",
    "**TODO:** Can you deploy your model to an endpoint and then query that endpoint to get a result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Deploy your model to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run an prediction on the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remember to shutdown/delete your endpoint once your work is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheaper Training and Cost Analysis\n",
    "**TODO:** Can you perform a cost analysis of your system and then use spot instances to lessen your model training cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your model using a spot instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Instance Training\n",
    "**TODO:** Can you train your model on multiple instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your model on Multiple Instances"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "sagemaker_py310",
   "language": "python",
   "name": "sagemaker_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
